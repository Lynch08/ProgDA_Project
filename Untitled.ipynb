{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4daaf75-bd07-4c96-a4a2-ec2d2d319e03",
   "metadata": {},
   "source": [
    "Lecturer: Dr Brian McGinley  \n",
    "Module: Programming For Data Analysis  \n",
    "Author: Enda Lynch  \n",
    "Github Username: Lynch08  \n",
    "GMIT Email: G003987951@gmit.ie  \n",
    "Personal Email: elyn@live.ie  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca18b60-7675-4f2f-80b9-e7b55b8f5ab8",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This repository contains all of the files pertaining to my 2021 project submission for the Programming for Data Analysis module of the GMIT H.Dip program in Data Analytics. All of the work contained within this repository was carried out over the course of a 3 week period in August 2022. This Jupyter notebook contains the complete documentation for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d6ab6-e989-4516-a92e-4efc3776d4dc",
   "metadata": {},
   "source": [
    "### 1.1 Project objective\n",
    "The objective of this project is to synthesise a data set based on some real world phenomenon. This requires investigation in to the phenomenon and then using the numpy.random package in Python to simulated some data based on this. The problem statement for the assignment is as follows :\n",
    "\n",
    "1. Choose a real-world phenomenon that can be measured and for which you could collect at least one-hundred data points across at least four different variables.  \n",
    "2. Investigate the types of variables involved, their likely distributions, and their relationships with each other.  \n",
    "3. Synthesise/simulate a data set as closely matching their properties as possible.  \n",
    "4. Detail your research and implement the simulation in a Jupyter notebook – the data set itself can simply be displayed in an output cell within the notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7c0aa-84c1-46ad-ad00-719ce0413fa8",
   "metadata": {},
   "source": [
    "### 1.2 Choice of real world phenomenon  \n",
    "The real world phenomenon I have chosen to analyse is the use of the social media app Tinder among millennials. As a person who has dabbled with online dating and has had mixed experiences that has led to mixed opinons on the use of online dating apps in general this was interesting to me.\n",
    "<br>\n",
    "The dataset I have chosen to synthesise is the Tinder Millennial Match Rate Dataset. \n",
    "I obtained this data from the website kaggle.com. Here is the link to the page: https://www.kaggle.com/datasets/benroshan/tinder-millennial-match-rate\n",
    "<br>\n",
    "Although online dating has been around pretty much since the internet became mainstream, since the advent of the smart phone it has become extremly common and accepted by society as a whole. Also with the recent covid-19 pandemic that led to multiple lockdowns the number of users increased significally and as you would imagine as people were not able to socialise in person, the people who had being using these apps pre lockdown were using them even more. In fact in 2021 \"Tinder says its users had 11 per cent more swipes and 42 per cent more matches last year, making 2020 the app’s busiest year\".\n",
    "The data set I am using to investigate is not exploring the user rates, but instead it is the data measuring the \"success\" rate via a poll taken by students from different Universities around the US and asked one question \"Have you ever met up with someone off tinder?\". There are 3 possible answers \"Yes\", \"No\" or \"I don't use Tinder\". It also contains the data of how many males and females wer polled, which can also lead to interesting analysis. It then gives the percentage of matches for the respondents of a specific university or gender and finally, it gives a simple yes or no anser as  \n",
    "I choose this data set as it contains it may contain an interesting insight into whether males and although the primary goal of this project is to attempt to synthasise a dataset with similar distributions and properties, I may learn something of nutritional value along the way.  \n",
    "\n",
    "\n",
    "\n",
    "Cereals are an intersting case study as for many cerals are seen as one of the staples of a healthy diet, when in actual fact, the exact opposite can be true depending on your choice as many are loaded with hidden sugars that the advertising and the smiling person in the advertisment do not tell you. \n",
    "For me, when I was growing up I often used a bowl of cereal as a snack at any time of the day, this continued well into adulthood and I suppose I really can only blame myself for thinking that I was actually choosing the \"healthy\" option when I was loading up the bowl with something called \"Sugar Puffs\", \"Frosted Wheats\" and/or \"Coco Pops\". \n",
    "I choose this data set as it contains the nutritional information of 77 different brands of cereal, and although the primary goal of this project is to attempt to synthasise a dataset with similar distributions and properties, I may learn something of nutritional value along the way.  \n",
    "<br>\n",
    "There are a number of variables that must be considerd when looking at the nutritional value of any food and cereals are no different. The obvious nutritional information many people look for when they pick up a package of food is calorie count, sugars, fat, carbohydrates and to a lesser extent in my personal opion fiber and protein(unless you are attempting to build muscle - a phase I got into in my late 20's, forgotten now). This information is provided for each cereal and there is some more, including an overall rating. It is not clear from the original dataset or kaggle how this rating is determined, however it is suggested it may be from consumer reports. One of my tasks during the analysis phase will be to see does this rating corrolate with any of the other data. Information such as if the cereal is eaten hot or cold, and what shelf the cereal is normally displayed on is also provided, this will be interesting to see if the high sugar content cereals aimed primarily at children are placed on the lower shelves. I will go through each column from the original dataset in a section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb8b87-5f9e-4af0-b16d-70e9881c5f6e",
   "metadata": {},
   "source": [
    "# 1.3 Investigate the Variables\n",
    "I will investigate the types of variables of the original dataset, their likely distributions, and their relationships with each other in an attemt to understand the data better for synthesis.  \n",
    "<br>\n",
    "I will attempt to analyse the data using tools from differnt python libaries such as pandas, matplotlib and seaborn. Some of these tools will be used to read in the data and provide statistical information while others will generate plots such as pairplots, histograms and scatter plots in order to attempt to determine distributions of the data types and corrolations between some of the factors. Because there are 16 columns of data I am sure I will miss some of these corrolations. However I will try to highlight the obvious ones during the analysis phase and replicate those corrolations in my synthisised data set. I may even leave out trying to simulate some of the data from the original datasets if I feel that it is getting too complex.  \n",
    "<br>\n",
    "However the goal from the outset is to attempt to simulate a dataset that is as close to the real world scenario as possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f74116-1405-477a-bb6d-abf10b9128a4",
   "metadata": {},
   "source": [
    "# 1.4 Synthesising the data\n",
    "After analysing the original data set I will then use python, specifically the numpy.random package to simulate the data. From my analysis I will consider what distributions I should be using for each variable and how they can be simulated using the relavent tools from numpy. While simulating data for a single variable is not a hugely difficult ask, I believe that choosing the correct distribution type and then corrolating that distribution with another variable will be challenging. I have some expierence here and I know that some of the online videos, including the course guides make it look a little easier than this actually is.  \n",
    "<br>\n",
    "After the analysis I will be highlighting the most relevent distributions and corrolations that jump out at me and trying to simulate this in the synthisised data set. For example some of the things I will be looking to see are:\n",
    " - Is there is a corrolation between calorie count and sugars?  \n",
    " - Is there a corrolation between sugars and the shelve where the cereal box is displayed? \n",
    " - How does the dat and sugar volume corrolate with the overall rating?\n",
    " - Is there any obvious trends/corrolations that tie a manufacturer to a specifc nutritional type?  \n",
    "<br>\n",
    "\n",
    "These questions any many more like them will be the types of things I will be asking myself when deciding on the factors I will need to consider when synthesising the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c99003-e52b-4fb1-a458-a7e5532f2457",
   "metadata": {},
   "source": [
    "# 1.5 Detailing the research\n",
    "\n",
    "The body of this project will be detailed in this Jupyter notebook. That will include the analysis of the original dataset and the method and synthisis of the simulated data. As mentioned above the analysis will include statistical and visual tools to guide me in the synthisis phase. All of the code will be available to run via this notebook and that will be stored in a github repository along with a copy of the original data set and any other relevent documents/images/data/resources that are used in the creation of this project.  \n",
    "This repository will also contain any cleaned data and all the files used will be clearly referenced in the notebook where relevent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eada2b1-393d-49ca-8c2e-272fec005846",
   "metadata": {},
   "source": [
    "# 1.6 Data Simulation\n",
    "\n",
    "The primary goal that this project has set is to create a dataset and investigate the \"types of variables involved, their likely distributions, and their relationships with each other.\" Although I have chosen a  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c1964-3a36-409a-bff6-2ead71b33277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d018cd-7c4e-436b-8368-dc49e2ef1daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88ac81-576e-4669-83a0-5c91d62b522a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d98d7-64c4-4f77-bacd-f0f764d70b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The original dataset I decided to attempt to syntesise has 16 columns. In the first 3 columns the data type is strings. These columns are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8431c6-584f-40ac-a11b-adf8d757d0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc172d-43dc-4939-b2b1-7071a7444f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3eee70-4be4-461b-82f8-3c52cde44e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d648f6a-1057-4470-b0fc-f0d1fbf5e198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501e7ea7-d9cc-4c20-ae52-ed932225262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of companies by appending number to 'country'\n",
    "company =[]\n",
    "# use for loop and concatenate number to string\n",
    "for i in range(80):\n",
    "    companyname = 'Sim_Company_'+str(i)\n",
    "    # append company  to list of countries\n",
    "    company.append(companyname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53cd46bd-203f-4ea8-84c6-6090b48053ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sim_Company_0',\n",
       " 'Sim_Company_1',\n",
       " 'Sim_Company_2',\n",
       " 'Sim_Company_3',\n",
       " 'Sim_Company_4',\n",
       " 'Sim_Company_5',\n",
       " 'Sim_Company_6',\n",
       " 'Sim_Company_7',\n",
       " 'Sim_Company_8',\n",
       " 'Sim_Company_9',\n",
       " 'Sim_Company_10',\n",
       " 'Sim_Company_11',\n",
       " 'Sim_Company_12',\n",
       " 'Sim_Company_13',\n",
       " 'Sim_Company_14',\n",
       " 'Sim_Company_15',\n",
       " 'Sim_Company_16',\n",
       " 'Sim_Company_17',\n",
       " 'Sim_Company_18',\n",
       " 'Sim_Company_19',\n",
       " 'Sim_Company_20',\n",
       " 'Sim_Company_21',\n",
       " 'Sim_Company_22',\n",
       " 'Sim_Company_23',\n",
       " 'Sim_Company_24',\n",
       " 'Sim_Company_25',\n",
       " 'Sim_Company_26',\n",
       " 'Sim_Company_27',\n",
       " 'Sim_Company_28',\n",
       " 'Sim_Company_29',\n",
       " 'Sim_Company_30',\n",
       " 'Sim_Company_31',\n",
       " 'Sim_Company_32',\n",
       " 'Sim_Company_33',\n",
       " 'Sim_Company_34',\n",
       " 'Sim_Company_35',\n",
       " 'Sim_Company_36',\n",
       " 'Sim_Company_37',\n",
       " 'Sim_Company_38',\n",
       " 'Sim_Company_39',\n",
       " 'Sim_Company_40',\n",
       " 'Sim_Company_41',\n",
       " 'Sim_Company_42',\n",
       " 'Sim_Company_43',\n",
       " 'Sim_Company_44',\n",
       " 'Sim_Company_45',\n",
       " 'Sim_Company_46',\n",
       " 'Sim_Company_47',\n",
       " 'Sim_Company_48',\n",
       " 'Sim_Company_49',\n",
       " 'Sim_Company_50',\n",
       " 'Sim_Company_51',\n",
       " 'Sim_Company_52',\n",
       " 'Sim_Company_53',\n",
       " 'Sim_Company_54',\n",
       " 'Sim_Company_55',\n",
       " 'Sim_Company_56',\n",
       " 'Sim_Company_57',\n",
       " 'Sim_Company_58',\n",
       " 'Sim_Company_59',\n",
       " 'Sim_Company_60',\n",
       " 'Sim_Company_61',\n",
       " 'Sim_Company_62',\n",
       " 'Sim_Company_63',\n",
       " 'Sim_Company_64',\n",
       " 'Sim_Company_65',\n",
       " 'Sim_Company_66',\n",
       " 'Sim_Company_67',\n",
       " 'Sim_Company_68',\n",
       " 'Sim_Company_69',\n",
       " 'Sim_Company_70',\n",
       " 'Sim_Company_71',\n",
       " 'Sim_Company_72',\n",
       " 'Sim_Company_73',\n",
       " 'Sim_Company_74',\n",
       " 'Sim_Company_75',\n",
       " 'Sim_Company_76',\n",
       " 'Sim_Company_77',\n",
       " 'Sim_Company_78',\n",
       " 'Sim_Company_79']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01fe5af-429c-4432-997e-e421ccab5ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6e9cc-258b-470e-993e-ec1cfbe1c5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d349c-9b2b-4e43-8f3a-94695ce370b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187beeee-ad35-4ac5-b009-a3c371edbf78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1c1a6-9ae1-4e4d-a859-c7d8a6c120e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bc7cb-b56d-4178-a195-92f82e1bd471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777cb7a-83da-4265-bf69-732239aa95e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a39be7-f076-4a00-8aea-bbe0255024e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20afa9c2-4ed0-401b-9489-a056db5a0faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f810adb-d2d0-4d28-b9a7-69798db8f69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c930a5ed-05e7-43a0-9214-4cf2c87cea25",
   "metadata": {},
   "source": [
    "https://theconversation.com/relationships-during-a-pandemic-how-dating-apps-have-adapted-to-covid-19-160219\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
